# PH√ÇN T√çCH TO√ÄN B·ªò FLOW "ADD SURVEY REPORT" - BACKEND V1 & BACKEND M·ªöI

## Ng√†y ph√¢n t√≠ch: 2025
## M·ª•c ƒë√≠ch: So s√°nh v√† x√°c minh vi·ªác migration t·ª´ backend-v1 sang backend m·ªõi (modular FastAPI)

---

## üìã M·ª§C L·ª§C

1. [T·ªïng quan Flow](#1-t·ªïng-quan-flow)
2. [Frontend Flow - React](#2-frontend-flow---react)
3. [Backend V1 Flow (Legacy)](#3-backend-v1-flow-legacy)
4. [Backend M·ªõi Flow (Migrated)](#4-backend-m·ªõi-flow-migrated)
5. [Endpoints ƒë∆∞·ª£c s·ª≠ d·ª•ng](#5-endpoints-ƒë∆∞·ª£c-s·ª≠-d·ª•ng)
6. [Functions & Helpers](#6-functions--helpers)
7. [Document AI Integration](#7-document-ai-integration)
8. [Targeted OCR Process](#8-targeted-ocr-process)
9. [PDF Splitting Logic (>15 pages)](#9-pdf-splitting-logic-15-pages)
10. [File Upload to Google Drive](#10-file-upload-to-google-drive)
11. [So s√°nh Backend V1 vs Backend M·ªõi](#11-so-s√°nh-backend-v1-vs-backend-m·ªõi)
12. [T·ªïng k·∫øt & Khuy·∫øn ngh·ªã](#12-t·ªïng-k·∫øt--khuy·∫øn-ngh·ªã)

---

## 1. T·ªîNG QUAN FLOW

### Quy tr√¨nh t·ªïng th·ªÉ:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ USER: Ch·ªçn t√†u ‚Üí Upload PDF file                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FRONTEND: G·ª≠i file ƒë·∫øn /api/survey-reports/analyze-file                ‚îÇ
‚îÇ - FormData: survey_report_file, ship_id, bypass_validation             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ BACKEND: Endpoint analyze-file                                          ‚îÇ
‚îÇ 1. Validate PDF file (magic bytes, size, format)                       ‚îÇ
‚îÇ 2. Check page count ‚Üí Quy·∫øt ƒë·ªãnh split hay kh√¥ng (>15 pages)           ‚îÇ
‚îÇ 3. Process with Document AI (Google Cloud)                             ‚îÇ
‚îÇ 4. Perform Targeted OCR (Tesseract + pdf2image)                        ‚îÇ
‚îÇ 5. Extract fields with System AI (LLM - GPT/Gemini)                    ‚îÇ
‚îÇ 6. Validate ship name/IMO (fuzzy matching)                             ‚îÇ
‚îÇ 7. Return: analysis data + base64 file content + summary               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FRONTEND: Nh·∫≠n analysis result                                          ‚îÇ
‚îÇ - Auto-fill form fields (survey_report_name, report_no, etc.)          ‚îÇ
‚îÇ - Store analyzed data (including _file_content, _summary_text)         ‚îÇ
‚îÇ - User c√≥ th·ªÉ ch·ªânh s·ª≠a th·ªß c√¥ng n·∫øu c·∫ßn                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FRONTEND: User nh·∫•n "Save" ‚Üí POST /api/survey-reports                  ‚îÇ
‚îÇ - Create record in database (without file IDs)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FRONTEND: Background upload                                             ‚îÇ
‚îÇ POST /api/survey-reports/{report_id}/upload-files                      ‚îÇ
‚îÇ - Body: file_content (base64), filename, content_type, summary_text    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ BACKEND: upload-files endpoint                                          ‚îÇ
‚îÇ 1. Decode base64 file content                                          ‚îÇ
‚îÇ 2. Upload original file to Google Drive:                               ‚îÇ
‚îÇ    ‚Üí ShipName/Class & Flag Cert/Class Survey Report/[filename]         ‚îÇ
‚îÇ 3. Upload summary text file to Google Drive:                           ‚îÇ
‚îÇ    ‚Üí SUMMARY/Class & Flag Document/[filename]_Summary.txt              ‚îÇ
‚îÇ 4. Update record with file IDs:                                        ‚îÇ
‚îÇ    - survey_report_file_id                                             ‚îÇ
‚îÇ    - survey_report_summary_file_id                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FRONTEND: Refresh list ‚Üí Show file icons                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 2. FRONTEND FLOW - REACT

### File quan tr·ªçng:
- **Component**: `/app/frontend/src/components/ClassSurveyReport/AddSurveyReportModal.jsx`
- **Service**: `/app/frontend/src/services/surveyReportService.js`

### C√°c b∆∞·ªõc trong Frontend:

#### A. File Upload & Analysis

```javascript
// AddSurveyReportModal.jsx

// Step 1: User upload file (drag & drop ho·∫∑c click)
const handleFileSelect = async (files) => {
  const fileArray = Array.from(files);
  
  // Validate: PDF only, max 50MB
  if (!file.name.endsWith('.pdf') || file.size > 50MB) {
    toast.error('Invalid file');
    return;
  }
  
  // Single file mode
  if (fileArray.length === 1) {
    setUploadedFile(fileArray[0]);
    await analyzeFile(fileArray[0]);
  }
  // Batch mode: multiple files
  else {
    onStartBatchProcessing(fileArray);
  }
};

// Step 2: Call backend analyze endpoint
const analyzeFile = async (file) => {
  // Check software expiry
  if (isSoftwareExpired) return;
  
  setIsAnalyzing(true);
  
  // API call v·ªõi FormData
  const response = await surveyReportService.analyzeFile(
    selectedShip.id,
    file,
    false // bypass_validation = false
  );
  
  const data = response.data;
  
  // Case 1: Ship validation error
  if (data.validation_error) {
    setShowValidationModal(true);
    setValidationData({
      extracted_ship_name: data.extracted_ship_name,
      extracted_ship_imo: data.extracted_ship_imo,
      expected_ship_name: data.expected_ship_name,
      expected_ship_imo: data.expected_ship_imo,
      file: file
    });
    return;
  }
  
  // Case 2: Success
  if (data.success && data.analysis) {
    processAnalysisSuccess(data.analysis, file);
  }
};

// Step 3: Auto-fill form v·ªõi analyzed data
const processAnalysisSuccess = (analysis, file) => {
  // Store complete analysis (including hidden fields)
  setAnalyzedData(analysis);
  
  // Extract form fields
  const newFormData = {
    survey_report_name: analysis.survey_report_name || '',
    report_form: analysis.report_form || '',
    survey_report_no: analysis.survey_report_no || '',
    issued_date: analysis.issued_date || '',
    issued_by: analysis.issued_by || '',
    status: analysis.status || 'Valid',
    note: analysis.note || '',
    surveyor_name: analysis.surveyor_name || ''
  };
  
  setFormData(newFormData);
  
  // Show notifications
  if (analysis._split_info?.was_split) {
    toast.info(`File split into ${analysis._split_info.chunks_count} chunks`);
  }
  
  if (analysis._ocr_info?.ocr_success) {
    toast.success('OCR enhancement applied');
  }
  
  toast.success('Analysis complete!');
};
```

#### B. Create Record & Upload Files

```javascript
// Step 4: User clicks Save
const handleSubmit = async (e) => {
  e.preventDefault();
  
  // Validate form
  if (!formData.survey_report_name.trim()) {
    toast.error('Please enter report name');
    return;
  }
  
  setIsSaving(true);
  
  // Create record (without file IDs)
  const reportData = {
    ship_id: selectedShip.id,
    survey_report_name: formData.survey_report_name.trim(),
    report_form: formData.report_form.trim() || null,
    survey_report_no: formData.survey_report_no.trim() || null,
    issued_date: formData.issued_date || null,
    issued_by: formData.issued_by.trim() || null,
    status: formData.status || 'Valid',
    note: formData.note.trim() || null,
    surveyor_name: formData.surveyor_name.trim() || null
  };
  
  const createResponse = await surveyReportService.create(reportData);
  const createdReport = createResponse.data;
  
  toast.success('Survey report added successfully');
  
  // Close modal
  onClose();
  
  // Refresh list (first time - without file icons)
  if (onReportAdded) {
    onReportAdded();
  }
  
  // Step 5: Background upload if file exists
  if (uploadedFile && analyzedData && createdReport.id) {
    if (analyzedData._file_content && analyzedData._filename) {
      uploadFilesInBackground(
        createdReport.id,
        analyzedData._file_content,  // base64 string
        analyzedData._filename,
        analyzedData._content_type,
        analyzedData._summary_text || ''
      );
    }
  }
};

// Step 6: Background upload to Google Drive
const uploadFilesInBackground = async (reportId, fileContent, filename, contentType, summaryText) => {
  setTimeout(async () => {
    try {
      toast.info('Uploading files to Google Drive...');
      
      await surveyReportService.uploadFiles(
        reportId,
        fileContent,   // base64
        filename,
        contentType,
        summaryText    // Enhanced summary with OCR
      );
      
      toast.success('File upload complete!');
      
      // Refresh list again to show file icons
      if (onReportAdded) {
        onReportAdded();
      }
    } catch (error) {
      console.error('Failed to upload file:', error);
      toast.error('File upload failed');
    }
  }, 100);
};
```

### Service Methods:

```javascript
// surveyReportService.js

export const surveyReportService = {
  // Analyze file with AI
  analyzeFile: async (shipId, reportFile, bypassValidation = false) => {
    const formData = new FormData();
    formData.append('ship_id', shipId);
    formData.append('survey_report_file', reportFile);
    formData.append('bypass_validation', bypassValidation ? 'true' : 'false');
    
    return api.post('/api/survey-reports/analyze-file', formData, {
      headers: { 'Content-Type': 'multipart/form-data' },
      timeout: 90000, // 90 seconds for large PDFs
    });
  },

  // Create survey report record
  create: async (reportData) => {
    return api.post('/api/survey-reports', reportData);
  },

  // Upload files to Google Drive
  uploadFiles: async (reportId, fileContent, filename, contentType, summaryText) => {
    return api.post(`/api/survey-reports/${reportId}/upload-files`, {
      file_content: fileContent,
      filename: filename,
      content_type: contentType,
      summary_text: summaryText
    }, {
      timeout: 60000,  // 60 seconds
    });
  }
};
```

---

## 3. BACKEND V1 FLOW (LEGACY)

### File quan tr·ªçng:
- **Main server**: `/app/backend-v1/server.py`
- **OCR processor**: `/app/backend-v1/targeted_ocr.py`
- **PDF splitter**: `/app/backend-v1/pdf_splitter.py`
- **Google Drive manager**: `/app/backend-v1/dual_apps_script_manager.py`

### Endpoints trong V1:

#### A. Endpoint: `POST /api/survey-reports/analyze-file` (Line 8594-9270)

**Input:**
- `survey_report_file`: UploadFile (PDF)
- `ship_id`: str
- `bypass_validation`: str ("true" ho·∫∑c "false")

**Process:**

```python
@api_router.post("/survey-reports/analyze-file")
async def analyze_survey_report_file(
    survey_report_file: UploadFile = File(...),
    ship_id: str = Form(...),
    bypass_validation: str = Form("false"),
    current_user: UserResponse = Depends(check_permission([...]))
):
    """
    Step 1: Read file v√† validate
    """
    file_content = await survey_report_file.read()
    filename = survey_report_file.filename
    
    # Validate PDF file
    if not filename.lower().endswith('.pdf'):
        raise HTTPException(400, "Only PDF files supported")
    
    if not file_content.startswith(b'%PDF'):
        raise HTTPException(400, "Invalid PDF format")
    
    """
    Step 2: Check if PDF needs splitting (>15 pages)
    """
    from pdf_splitter import PDFSplitter
    splitter = PDFSplitter(max_pages_per_chunk=12)
    
    total_pages = splitter.get_page_count(file_content)
    needs_split = splitter.needs_splitting(file_content)  # True if > 15 pages
    
    logger.info(f"PDF: {total_pages} pages, Split needed: {needs_split}")
    
    """
    Step 3: Get ship & company info
    """
    ship = await mongo_db.find_one("ships", {"id": ship_id})
    if not ship and not bypass_validation:
        raise HTTPException(404, "Ship not found")
    
    ship_name = ship.get("name", "Unknown Ship")
    ship_imo = ship.get("imo", "")
    
    """
    Step 4: Get AI configuration
    """
    ai_config_doc = await mongo_db.find_one("ai_config", {"id": "system_ai"})
    if not ai_config_doc:
        raise HTTPException(404, "AI configuration not found")
    
    document_ai_config = ai_config_doc.get("document_ai", {})
    if not document_ai_config.get("enabled"):
        raise HTTPException(400, "Document AI not enabled")
    
    # Validate Document AI config
    if not all([
        document_ai_config.get("project_id"),
        document_ai_config.get("processor_id")
    ]):
        raise HTTPException(400, "Incomplete Document AI configuration")
    
    """
    Step 5: Initialize result & store file content
    """
    analysis_result = {
        "survey_report_name": "",
        "report_form": "",
        "survey_report_no": "",
        "issued_by": "",
        "issued_date": "",
        "ship_name": "",
        "ship_imo": "",
        "surveyor_name": "",
        "note": "",
        "status": "Valid",
        "confidence_score": 0.0,
        "processing_method": "clean_analysis"
    }
    
    # CRITICAL: Store file content FIRST (for upload even if AI fails)
    analysis_result['_file_content'] = base64.b64encode(file_content).decode('utf-8')
    analysis_result['_filename'] = filename
    analysis_result['_content_type'] = survey_report_file.content_type
    analysis_result['_ship_name'] = ship_name
    analysis_result['_summary_text'] = ''
    
    """
    Step 6A: Process based on PDF size
    """
    from dual_apps_script_manager import create_dual_apps_script_manager
    dual_manager = create_dual_apps_script_manager(company_uuid)
    
    # CASE 1: Small PDF (‚â§ 15 pages) - Normal processing
    if not needs_split:
        logger.info("Processing single PDF (no split)")
        
        # Call Document AI (through dual_manager)
        analysis_only_result = await dual_manager.analyze_survey_report_only(
            file_content=file_content,
            filename=filename,
            content_type=survey_report_file.content_type,
            document_ai_config=document_ai_config
        )
        
        # Store split info
        analysis_result['_split_info'] = {
            'was_split': False,
            'total_pages': total_pages,
            'chunks_count': 1
        }
        
        if analysis_only_result.get('success'):
            ai_analysis = analysis_only_result.get('ai_analysis', {})
            summary_text = ai_analysis.get('data', {}).get('summary', '')
            
            # STEP: Perform Targeted OCR (ALWAYS, independent of Document AI)
            logger.info("Starting Targeted OCR for header/footer extraction...")
            
            from targeted_ocr import get_ocr_processor
            ocr_processor = get_ocr_processor()
            
            ocr_metadata = {
                'ocr_attempted': False,
                'ocr_success': False,
                'ocr_text_merged': False,
                'header_text_length': 0,
                'footer_text_length': 0
            }
            
            ocr_section = ""
            
            if ocr_processor.is_available():
                ocr_metadata['ocr_attempted'] = True
                
                # Extract from first page (page_num=0)
                ocr_result = ocr_processor.extract_from_pdf(file_content, page_num=0)
                
                if ocr_result.get('ocr_success'):
                    header_text = ocr_result.get('header_text', '').strip()
                    footer_text = ocr_result.get('footer_text', '').strip()
                    
                    ocr_metadata['ocr_success'] = True
                    ocr_metadata['header_text_length'] = len(header_text)
                    ocr_metadata['footer_text_length'] = len(footer_text)
                    
                    # Create OCR section
                    if header_text or footer_text:
                        ocr_section = "\n\n" + "="*60 + "\n"
                        ocr_section += "ADDITIONAL INFORMATION FROM HEADER/FOOTER (OCR Extraction)\n"
                        ocr_section += "="*60 + "\n\n"
                        
                        if header_text:
                            ocr_section += "=== HEADER TEXT (Top 15% of page) ===\n"
                            ocr_section += header_text + "\n\n"
                        
                        if footer_text:
                            ocr_section += "=== FOOTER TEXT (Bottom 15% of page) ===\n"
                            ocr_section += footer_text + "\n\n"
                        
                        ocr_section += "="*60
                        ocr_metadata['ocr_text_merged'] = True
            
            # Merge Document AI summary with OCR
            if summary_text:
                if ocr_section:
                    summary_text = summary_text + ocr_section
                    logger.info(f"Enhanced summary with OCR: {len(summary_text)} chars")
            else:
                # Document AI failed, use OCR only
                summary_text = ocr_section if ocr_section else ''
            
            analysis_result['_ocr_info'] = ocr_metadata
            
            # STEP: Extract fields from enhanced summary using System AI
            logger.info("Extracting survey report fields from enhanced summary...")
            
            ai_provider = ai_config_doc.get("provider", "google")
            ai_model = ai_config_doc.get("model", "gemini-2.0-flash")
            use_emergent_key = ai_config_doc.get("use_emergent_key", True)
            
            extracted_fields = await extract_survey_report_fields_from_summary(
                summary_text,  # Includes OCR text
                ai_provider,
                ai_model,
                use_emergent_key,
                filename
            )
            
            if extracted_fields:
                analysis_result.update(extracted_fields)
                analysis_result["processing_method"] = "analysis_only_no_upload"
                analysis_result['_summary_text'] = summary_text
                logger.info("System AI extraction completed successfully")
            else:
                analysis_result['_summary_text'] = summary_text
                analysis_result["processing_method"] = "extraction_failed"
    
    # CASE 2: Large PDF (> 15 pages) - Split and process
    else:
        logger.info(f"Splitting PDF ({total_pages} pages) into chunks...")
        chunks = splitter.split_pdf(file_content, filename)
        logger.info(f"Created {len(chunks)} chunks")
        
        # Process each chunk
        chunk_results = []
        for i, chunk in enumerate(chunks):
            logger.info(f"Processing chunk {i+1}/{len(chunks)} (pages {chunk['page_range']})")
            
            chunk_analysis = await dual_manager.analyze_survey_report_only(
                file_content=chunk['content'],
                filename=chunk['filename'],
                content_type='application/pdf',
                document_ai_config=document_ai_config
            )
            
            # Collect summaries only (no field extraction yet)
            if chunk_analysis.get('success'):
                ai_analysis = chunk_analysis.get('ai_analysis', {})
                if ai_analysis.get('success'):
                    summary_text = ai_analysis.get('data', {}).get('summary', '')
                    
                    if summary_text:
                        chunk_results.append({
                            'success': True,
                            'chunk_num': chunk['chunk_num'],
                            'page_range': chunk['page_range'],
                            'summary_text': summary_text
                        })
        
        # Merge summaries
        logger.info("Merging summaries from all chunks...")
        successful_chunks = [cr for cr in chunk_results if cr.get('success')]
        
        if successful_chunks:
            from pdf_splitter import create_enhanced_merged_summary
            
            # Create temporary merged data
            temp_merged_data = {
                'survey_report_name': 'Processing...',
                'survey_report_no': 'Processing...',
                'issued_by': 'Processing...',
                'issued_date': 'Processing...',
                'surveyor_name': 'Processing...',
                'status': 'Valid',
                'note': ''
            }
            
            merged_summary_text = create_enhanced_merged_summary(
                chunk_results=chunk_results,
                merged_data=temp_merged_data,
                original_filename=filename,
                total_pages=total_pages
            )
            
            logger.info(f"Created merged summary ({len(merged_summary_text)} chars)")
            
            # Perform Targeted OCR on FIRST CHUNK
            logger.info("Starting Targeted OCR on FIRST CHUNK...")
            
            ocr_metadata = {
                'ocr_attempted': False,
                'ocr_success': False,
                'ocr_text_merged': False,
                'header_text_length': 0,
                'footer_text_length': 0,
                'note': 'OCR performed on first chunk only'
            }
            
            ocr_section = ""
            
            if ocr_processor.is_available() and chunks:
                first_chunk_content = chunks[0]['content']
                ocr_result = ocr_processor.extract_from_pdf(first_chunk_content, page_num=0)
                
                if ocr_result.get('ocr_success'):
                    header_text = ocr_result.get('header_text', '').strip()
                    footer_text = ocr_result.get('footer_text', '').strip()
                    
                    ocr_metadata['ocr_success'] = True
                    ocr_metadata['header_text_length'] = len(header_text)
                    ocr_metadata['footer_text_length'] = len(footer_text)
                    
                    if header_text or footer_text:
                        ocr_section = "\n\n" + "="*60 + "\n"
                        ocr_section += "ADDITIONAL INFORMATION FROM HEADER/FOOTER (OCR Extraction)\n"
                        ocr_section += "(Extracted from first page of document)\n"
                        ocr_section += "="*60 + "\n\n"
                        
                        if header_text:
                            ocr_section += "=== HEADER TEXT (Top 15% of page) ===\n"
                            ocr_section += header_text + "\n\n"
                        
                        if footer_text:
                            ocr_section += "=== FOOTER TEXT (Bottom 15% of page) ===\n"
                            ocr_section += footer_text + "\n\n"
                        
                        ocr_section += "="*60
                        ocr_metadata['ocr_text_merged'] = True
                        
                        # Merge OCR into merged summary
                        merged_summary_text = merged_summary_text + ocr_section
            
            # Extract fields from merged summary (1 time only)
            logger.info("Extracting fields from MERGED SUMMARY (System AI)...")
            
            ai_provider = ai_config_doc.get("provider", "google")
            ai_model = ai_config_doc.get("model", "gemini-2.0-flash")
            use_emergent_key = ai_config_doc.get("use_emergent_key", True)
            
            extracted_fields = await extract_survey_report_fields_from_summary(
                merged_summary_text,
                ai_provider,
                ai_model,
                use_emergent_key,
                filename
            )
            
            if extracted_fields:
                # Recreate summary with actual extracted data
                final_merged_summary = create_enhanced_merged_summary(
                    chunk_results=chunk_results,
                    merged_data=extracted_fields,
                    original_filename=filename,
                    total_pages=total_pages
                )
                
                analysis_result.update(extracted_fields)
                analysis_result["processing_method"] = "split_pdf_batch_processing"
                analysis_result['_ocr_info'] = ocr_metadata
                analysis_result['_split_info'] = {
                    'was_split': True,
                    'total_pages': total_pages,
                    'chunks_count': len(chunks),
                    'successful_chunks': len(successful_chunks)
                }
                analysis_result['_summary_text'] = final_merged_summary
                
                logger.info("Split PDF processing complete!")
    
    """
    Step 7: Validate ship name/IMO
    """
    extracted_ship_name = analysis_result.get('ship_name', '').strip()
    extracted_ship_imo = analysis_result.get('ship_imo', '').strip()
    
    if extracted_ship_name or extracted_ship_imo:
        validation_result = validate_ship_info_match(
            extracted_ship_name,
            extracted_ship_imo,
            ship_name,
            ship_imo
        )
        
        should_bypass = bypass_validation.lower() == "true"
        
        if not validation_result.get('overall_match') and not should_bypass:
            # Return validation error for frontend to handle
            return {
                "success": False,
                "validation_error": True,
                "validation_details": validation_result,
                "message": "Ship information mismatch. Please verify or bypass validation.",
                "extracted_ship_name": extracted_ship_name,
                "extracted_ship_imo": extracted_ship_imo,
                "expected_ship_name": ship_name,
                "expected_ship_imo": ship_imo,
                "split_info": analysis_result.get('_split_info')
            }
    
    """
    Step 8: Return analysis result
    """
    return {
        "success": True,
        "analysis": analysis_result,
        "ship_name": ship_name,
        "ship_imo": ship_imo
    }
```

#### B. Endpoint: `POST /api/survey-reports/{report_id}/upload-files` (Line 9334-9465)

```python
@api_router.post("/survey-reports/{report_id}/upload-files")
async def upload_survey_report_files(
    report_id: str,
    file_content: str = Body(...),
    filename: str = Body(...),
    content_type: str = Body(...),
    summary_text: str = Body(...),
    current_user: UserResponse = Depends(check_permission([...]))
):
    """
    Upload survey report files to Google Drive after record creation
    1. Decode base64 file content
    2. Upload original file to: ShipName/Class & Flag Cert/Class Survey Report/
    3. Upload summary to: SUMMARY/Class & Flag Document/
    4. Update survey report record with file IDs
    """
    
    # Validate report exists
    report = await mongo_db.find_one("survey_reports", {"id": report_id})
    if not report:
        raise HTTPException(404, "Survey report not found")
    
    # Get ship info
    ship_id = report.get("ship_id")
    ship = await mongo_db.find_one("ships", {"id": ship_id})
    if not ship:
        raise HTTPException(404, "Ship not found")
    
    ship_name = ship.get("name", "Unknown Ship")
    survey_report_name = report.get("survey_report_name", "Survey Report")
    
    # Decode base64 file content
    try:
        file_bytes = base64.b64decode(file_content)
        logger.info(f"Decoded file content: {len(file_bytes)} bytes")
    except Exception as e:
        raise HTTPException(400, "Invalid file content encoding")
    
    # Initialize Dual Apps Script Manager
    from dual_apps_script_manager import create_dual_apps_script_manager
    dual_manager = create_dual_apps_script_manager(company_uuid)
    
    upload_results = {}
    
    # Upload 1: Original file to ShipName/Class & Flag Cert/Class Survey Report/
    logger.info(f"Uploading original file to: {ship_name}/Class & Flag Cert/Class Survey Report/{filename}")
    
    original_upload = await dual_manager.upload_survey_report_file(
        file_content=file_bytes,
        filename=filename,
        content_type=content_type,
        ship_name=ship_name,
        survey_report_name=survey_report_name
    )
    
    if original_upload.get('success'):
        survey_report_file_id = original_upload.get('survey_report_file_id')
        upload_results['original'] = original_upload
        logger.info(f"Original file uploaded: {survey_report_file_id}")
    else:
        raise HTTPException(500, "Failed to upload survey report file")
    
    # Upload 2: Summary file to SUMMARY/Class & Flag Document/
    survey_report_summary_file_id = None
    if summary_text and summary_text.strip():
        base_name = filename.rsplit('.', 1)[0]
        summary_filename = f"{base_name}_Summary.txt"
        
        logger.info(f"Uploading summary file to: SUMMARY/Class & Flag Document/{summary_filename}")
        
        summary_upload = await dual_manager.upload_survey_report_summary(
            summary_text=summary_text,
            filename=summary_filename,
            ship_name=ship_name
        )
        
        if summary_upload.get('success'):
            survey_report_summary_file_id = summary_upload.get('summary_file_id')
            upload_results['summary'] = summary_upload
            logger.info(f"Summary file uploaded: {survey_report_summary_file_id}")
    
    # Update survey report record with file IDs
    update_data = {
        "survey_report_file_id": survey_report_file_id,
        "updated_at": datetime.now(timezone.utc)
    }
    
    if survey_report_summary_file_id:
        update_data["survey_report_summary_file_id"] = survey_report_summary_file_id
    
    await mongo_db.update("survey_reports", {"id": report_id}, update_data)
    logger.info("Survey report record updated with file IDs")
    
    return {
        "success": True,
        "survey_report_file_id": survey_report_file_id,
        "survey_report_summary_file_id": survey_report_summary_file_id,
        "message": "Survey report files uploaded successfully",
        "upload_results": upload_results
    }
```

---

## 4. BACKEND M·ªöI FLOW (MIGRATED)

### File quan tr·ªçng:
- **API Routes**: `/app/backend/app/api/v1/survey_reports.py`
- **Analysis Service**: `/app/backend/app/services/survey_report_analyze_service.py`
- **Survey Report Service**: `/app/backend/app/services/survey_report_service.py`
- **AI Utils**: `/app/backend/app/utils/survey_report_ai.py`
- **PDF Splitter**: `/app/backend/app/utils/pdf_splitter.py`
- **Models**: `/app/backend/app/models/survey_report.py`

### C·∫•u tr√∫c modular:

```
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/v1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ survey_reports.py          # API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ survey_report_service.py   # CRUD operations
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ survey_report_analyze_service.py  # AI analysis logic
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ survey_report_ai.py        # LLM extraction prompt & logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pdf_splitter.py            # PDF splitting utility
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ survey_report.py           # Pydantic models
```

### Endpoints trong Backend M·ªõi:

#### A. Endpoint: `POST /api/survey-reports/analyze-file`

```python
# File: survey_reports.py

@router.post("/analyze-file")
async def analyze_survey_report_file(
    survey_report_file: UploadFile = File(...),
    ship_id: str = Form(...),
    bypass_validation: str = Form("false"),
    current_user: UserResponse = Depends(check_editor_permission)
):
    """
    Analyze survey report file using Google Document AI
    
    Delegates to SurveyReportAnalyzeService
    """
    from app.services.survey_report_analyze_service import SurveyReportAnalyzeService
    
    try:
        bypass_bool = bypass_validation.lower() == "true"
        
        result = await SurveyReportAnalyzeService.analyze_file(
            file=survey_report_file,
            ship_id=ship_id,
            bypass_validation=bypass_bool,
            current_user=current_user
        )
        
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error analyzing survey report file: {e}")
        raise HTTPException(500, f"Failed to analyze survey report: {str(e)}")
```

**SurveyReportAnalyzeService.analyze_file()** - T∆∞∆°ng t·ª± nh∆∞ V1:

```python
# File: survey_report_analyze_service.py

class SurveyReportAnalyzeService:
    @staticmethod
    async def analyze_file(
        file: UploadFile,
        ship_id: str,
        bypass_validation: bool,
        current_user: UserResponse
    ) -> Dict[str, Any]:
        """
        Process t∆∞∆°ng t·ª± V1:
        1. Validate PDF
        2. Check page count
        3. Process with Document AI
        4. Perform Targeted OCR
        5. Extract fields with System AI
        6. Validate ship name/IMO
        7. Return analysis + file content
        """
        
        # Step 1: Read & validate
        file_content = await file.read()
        filename = file.filename
        
        # Validate PDF
        if not filename.lower().endswith('.pdf'):
            raise HTTPException(400, "Only PDF files supported")
        
        if not file_content.startswith(b'%PDF'):
            raise HTTPException(400, "Invalid PDF format")
        
        # Step 2: Check page count
        splitter = PDFSplitter(max_pages_per_chunk=12)
        total_pages = splitter.get_page_count(file_content)
        needs_split = splitter.needs_splitting(file_content)
        
        # Step 3: Get ship & AI config
        ship = await mongo_db.find_one("ships", {"id": ship_id})
        ai_config_doc = await mongo_db.find_one("ai_config", {"id": "system_ai"})
        
        # Step 4: Initialize result
        analysis_result = {
            # ... same fields as V1 ...
        }
        
        # Store file content
        analysis_result['_file_content'] = base64.b64encode(file_content).decode('utf-8')
        analysis_result['_filename'] = filename
        analysis_result['_content_type'] = file.content_type
        analysis_result['_ship_name'] = ship_name
        analysis_result['_summary_text'] = ''
        
        # Step 5: Process based on size
        if not needs_split:
            # Small PDF - call _process_single_pdf()
            analysis_result = await SurveyReportAnalyzeService._process_single_pdf(
                file_content,
                filename,
                file.content_type,
                document_ai_config,
                ai_config_doc,
                analysis_result,
                total_pages
            )
        else:
            # Large PDF - call _process_large_pdf()
            analysis_result = await SurveyReportAnalyzeService._process_large_pdf(
                file_content,
                filename,
                splitter,
                document_ai_config,
                ai_config_doc,
                analysis_result,
                total_pages
            )
        
        # Step 6: Validate ship name/IMO (fuzzy matching)
        if not bypass_validation:
            # ... validation logic ...
            pass
        
        # Step 7: Return result
        return {
            "success": True,
            "analysis": analysis_result,
            "ship_name": ship_name,
            "ship_imo": ship_imo
        }
```

#### B. Endpoint: `POST /api/survey-reports/{report_id}/upload-files`

```python
# File: survey_reports.py

@router.post("/{report_id}/upload-files")
async def upload_survey_report_files(
    report_id: str,
    file_content: str = Body(...),
    filename: str = Body(...),
    content_type: str = Body(...),
    summary_text: str = Body(...),
    current_user: UserResponse = Depends(check_editor_permission)
):
    """Upload survey report files to Google Drive"""
    from app.services.survey_report_service import SurveyReportService
    
    try:
        result = await SurveyReportService.upload_files(
            report_id=report_id,
            file_content=file_content,
            filename=filename,
            content_type=content_type,
            summary_text=summary_text,
            current_user=current_user
        )
        
        return result
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error uploading survey report files: {e}")
        raise HTTPException(500, f"Failed to upload files: {str(e)}")
```

**SurveyReportService.upload_files()** - T∆∞∆°ng t·ª± V1:

```python
# File: survey_report_service.py

class SurveyReportService:
    @staticmethod
    async def upload_files(
        report_id: str,
        file_content: str,
        filename: str,
        content_type: str,
        summary_text: str,
        current_user: UserResponse
    ) -> Dict[str, Any]:
        """
        Upload survey report files to Google Drive
        1. Validate report exists
        2. Decode base64 file content
        3. Upload original file to Google Drive
        4. Upload summary file to Google Drive
        5. Update record with file IDs
        """
        
        # Validate report
        report = await mongo_db.find_one("survey_reports", {"id": report_id})
        if not report:
            raise HTTPException(404, "Survey report not found")
        
        # Get ship info
        ship_id = report.get("ship_id")
        ship = await mongo_db.find_one("ships", {"id": ship_id})
        
        ship_name = ship.get("name", "Unknown Ship")
        survey_report_name = report.get("survey_report_name", "Survey Report")
        
        # Decode base64
        try:
            file_bytes = base64.b64decode(file_content)
        except Exception as e:
            raise HTTPException(400, "Invalid file content encoding")
        
        # Get company UUID for Google Drive manager
        company_uuid = current_user.company
        
        from app.services.gdrive_service import GoogleDriveService
        
        # Upload 1: Original file
        original_upload = await GoogleDriveService.upload_survey_report_file(
            company_uuid=company_uuid,
            file_content=file_bytes,
            filename=filename,
            content_type=content_type,
            ship_name=ship_name,
            survey_report_name=survey_report_name
        )
        
        if not original_upload.get('success'):
            raise HTTPException(500, "Failed to upload original file")
        
        survey_report_file_id = original_upload.get('survey_report_file_id')
        
        # Upload 2: Summary file
        survey_report_summary_file_id = None
        if summary_text and summary_text.strip():
            base_name = filename.rsplit('.', 1)[0]
            summary_filename = f"{base_name}_Summary.txt"
            
            summary_upload = await GoogleDriveService.upload_survey_report_summary(
                company_uuid=company_uuid,
                summary_text=summary_text,
                filename=summary_filename,
                ship_name=ship_name
            )
            
            if summary_upload.get('success'):
                survey_report_summary_file_id = summary_upload.get('summary_file_id')
        
        # Update record
        from datetime import datetime, timezone
        update_data = {
            "survey_report_file_id": survey_report_file_id,
            "updated_at": datetime.now(timezone.utc)
        }
        
        if survey_report_summary_file_id:
            update_data["survey_report_summary_file_id"] = survey_report_summary_file_id
        
        await mongo_db.update("survey_reports", {"id": report_id}, update_data)
        
        return {
            "success": True,
            "survey_report_file_id": survey_report_file_id,
            "survey_report_summary_file_id": survey_report_summary_file_id,
            "message": "Files uploaded successfully"
        }
```

---

## 5. ENDPOINTS ƒê∆Ø·ª¢C S·ª¨ D·ª§NG

### API Endpoints Summary:

| Endpoint | Method | Purpose | Input | Output |
|----------|--------|---------|-------|--------|
| `/api/survey-reports/analyze-file` | POST | Ph√¢n t√≠ch file PDF v·ªõi AI | `survey_report_file` (PDF)<br>`ship_id` (string)<br>`bypass_validation` (string) | `{success: bool, analysis: {...}, ship_name, ship_imo}` |
| `/api/survey-reports` | POST | T·∫°o record survey report | `{ship_id, survey_report_name, report_form, ...}` | `{id, survey_report_name, ...}` |
| `/api/survey-reports/{report_id}/upload-files` | POST | Upload files l√™n Google Drive | `{file_content (base64), filename, content_type, summary_text}` | `{success: bool, survey_report_file_id, survey_report_summary_file_id}` |
| `/api/survey-reports/{report_id}` | DELETE | X√≥a survey report + files | `report_id` | `{success: bool, message}` |
| `/api/survey-reports/bulk-delete` | POST | X√≥a nhi·ªÅu survey reports | `{report_ids: [...]}` | `{success: bool, deleted_count}` |
| `/api/survey-reports/check-duplicate` | POST | Ki·ªÉm tra tr√πng l·∫∑p | `{ship_id, survey_report_name, survey_report_no}` | `{is_duplicate: bool, existing_report}` |

---

## 6. FUNCTIONS & HELPERS

### A. Extract Survey Report Fields t·ª´ Summary

**Function**: `extract_survey_report_fields_from_summary()`

**Location V1**: `/app/backend-v1/server.py` (line 7413-7559)
**Location M·ªõi**: `/app/backend/app/utils/survey_report_ai.py`

**Purpose**: D√πng System AI (LLM) ƒë·ªÉ extract c√°c tr∆∞·ªùng t·ª´ summary text

**Input:**
- `summary_text`: str (Document AI summary + OCR text)
- `ai_provider`: str ("google", "openai", "anthropic")
- `ai_model`: str ("gemini-2.0-flash", "gpt-5", etc.)
- `use_emergent_key`: bool
- `filename`: str (ƒë·ªÉ extract report_form t·ª´ filename)

**Output:**
```python
{
    "survey_report_name": "cargo gear",  # ONLY equipment name, NO survey type
    "report_form": "CG (02-19)",
    "survey_report_no": "A/25/772",
    "issued_by": "Lloyd's Register",
    "issued_date": "2024-01-15",  # ISO format
    "ship_name": "MV Example Ship",
    "ship_imo": "9876543",
    "surveyor_name": "John Smith",
    "note": "All items found satisfactory"
}
```

**Key Logic:**

```python
async def extract_survey_report_fields_from_summary(
    summary_text: str,
    ai_provider: str,
    ai_model: str,
    use_emergent_key: bool,
    filename: str = ""
) -> dict:
    """
    Extract survey report fields from Document AI summary using System AI
    """
    
    # Step 1: Create extraction prompt
    prompt = create_survey_report_extraction_prompt(summary_text, filename)
    
    # Step 2: Call LLM
    if use_emergent_key and ai_provider in ["google", "emergent"]:
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        emergent_key = get_emergent_llm_key()
        chat = LlmChat(
            api_key=emergent_key,
            session_id=f"survey_extraction_{int(time.time())}",
            system_message="You are a maritime survey report analysis expert."
        ).with_model("gemini", ai_model)
        
        user_message = UserMessage(text=prompt)
        ai_response = await chat.send_message(user_message)
        
        # Step 3: Parse JSON response
        clean_content = ai_response.strip().replace('```json', '').replace('```', '').strip()
        extracted_data = json.loads(clean_content)
        
        # Step 4: Post-processing
        
        # 4A: Validate issued_date (check if it's a form code, not date)
        if extracted_data.get('issued_date'):
            issued_date_raw = extracted_data['issued_date'].strip()
            
            # Pattern: CU (02/19) is a FORM, not a DATE
            form_pattern = r'^[A-Z]{1,3}\s*\([0-9]{2}[/-][0-9]{2}\)$'
            
            if re.match(form_pattern, issued_date_raw):
                logger.warning(f"issued_date '{issued_date_raw}' looks like a Form, moving to report_form")
                if not extracted_data.get('report_form'):
                    extracted_data['report_form'] = issued_date_raw
                extracted_data['issued_date'] = ''
            else:
                try:
                    # Parse and convert to ISO format
                    from dateutil import parser
                    parsed_date = parser.parse(issued_date_raw)
                    extracted_data['issued_date'] = parsed_date.strftime('%Y-%m-%d')
                except Exception:
                    extracted_data['issued_date'] = ''
        
        # 4B: Format survey_report_name to Title Case
        if extracted_data.get('survey_report_name'):
            name = extracted_data['survey_report_name'].strip()
            
            # Remove survey type words
            words_to_remove = ['survey', 'report', 'record', 'annual', 'special', ...]
            words = name.lower().split()
            filtered_words = [w for w in words if w not in words_to_remove]
            
            if filtered_words:
                name = ' '.join(word.capitalize() for word in filtered_words)
                extracted_data['survey_report_name'] = name
        
        # 4C: Extract report_form from filename if AI didn't find it
        if not extracted_data.get('report_form') and filename:
            filename_form_patterns = [
                r'([A-Z]{1,3})\s*\(([0-9]{2}[-/][0-9]{2})\)',  # CG (02-19)
                r'([A-Z]{1,3})\s+([0-9]{2}[-/][0-9]{2})',      # CG 02-19
            ]
            
            for pattern in filename_form_patterns:
                match = re.search(pattern, filename)
                if match:
                    abbrev = match.group(1)
                    date_part = match.group(2).replace('/', '-')
                    extracted_form = f"{abbrev} ({date_part})"
                    extracted_data['report_form'] = extracted_form
                    break
        
        return extracted_data
    
    return {}
```

### B. Create Survey Report Extraction Prompt

**Function**: `create_survey_report_extraction_prompt()`

**Purpose**: T·∫°o AI prompt chi ti·∫øt ƒë·ªÉ extract c√°c tr∆∞·ªùng

**Key points trong prompt:**

```
=== FIELD EXTRACTION RULES ===

**survey_report_name**: 
- CRITICAL: Extract ONLY the equipment/system, NOT survey type
- REMOVE: "Annual", "Special", "Close-up", "Survey", "Report", "Record"
- ONLY extract the SUBJECT:
  * "survey record for cargo gear" ‚Üí "cargo gear"
  * "close-up survey of ballast tanks" ‚Üí "ballast tanks"
  * "annual survey of main engine" ‚Üí "main engine"

**report_form**: 
- CRITICAL 1: Check FILENAME first - often contains report form
  * Filename "CG (02-19).pdf" ‚Üí Report Form is "CG (02-19)"
- CRITICAL 2: Often in HEADER or FOOTER sections
- Common patterns: "CU (02/19)", "AS (03/20)", "CG (02-19)"
- Priority: Filename > Header/Footer > Document body

**issued_date**: 
- ONLY extract if it's clearly a DATE (e.g., "15 January 2024")
- DO NOT extract form codes that look like dates (e.g., "CU (02/19)")
- If uncertain, leave issued_date EMPTY and put value in report_form

**ship_name**: 
- Extract vessel name
- May include prefixes like "MV", "M/V", "MT", "M/T"

**ship_imo**: 
- Extract IMO number (7 digits)
- Format: "9876543" or "IMO 9876543"
```

### C. Validate Ship Info Match

**Function**: `validate_ship_info_match()`

**Purpose**: Fuzzy matching gi·ªØa extracted ship name/IMO v√† actual ship

**Logic:**

```python
def validate_ship_info_match(
    extracted_ship_name: str,
    extracted_ship_imo: str,
    actual_ship_name: str,
    actual_ship_imo: str
) -> Dict[str, Any]:
    """
    Validate if extracted ship info matches selected ship
    Returns validation results with match status
    """
    
    # Normalize names for comparison
    def normalize_ship_name(name: str) -> str:
        import re
        # Remove special characters
        name = re.sub(r'[^\w\s]', '', name)
        # Collapse spaces
        name = re.sub(r'\s+', ' ', name)
        # Remove common prefixes (M/V, M/T)
        name = re.sub(r'^(M/?V|M/?T)\s+', '', name, flags=re.IGNORECASE)
        return name.upper().strip()
    
    # Normalize IMO (extract 7 digits only)
    def normalize_imo(imo: str) -> str:
        import re
        digits = re.findall(r'\d{7}', imo)
        return digits[0] if digits else ''
    
    # Normalize values
    extracted_name_norm = normalize_ship_name(extracted_ship_name)
    actual_name_norm = normalize_ship_name(actual_ship_name)
    
    extracted_imo_norm = normalize_imo(extracted_ship_imo)
    actual_imo_norm = normalize_imo(actual_ship_imo)
    
    # Check matches
    name_match = (extracted_name_norm == actual_name_norm)
    imo_match = (extracted_imo_norm == actual_imo_norm)
    
    # Calculate fuzzy similarity for names
    from difflib import SequenceMatcher
    similarity = SequenceMatcher(None, extracted_name_norm, actual_name_norm).ratio()
    
    # Overall match (either name match OR IMO match OR high similarity)
    overall_match = name_match or imo_match or (similarity > 0.8)
    
    return {
        'name_match': name_match,
        'imo_match': imo_match,
        'similarity': similarity,
        'overall_match': overall_match,
        'extracted_name_normalized': extracted_name_norm,
        'actual_name_normalized': actual_name_norm,
        'extracted_imo_normalized': extracted_imo_norm,
        'actual_imo_normalized': actual_imo_norm
    }
```

---

## 7. DOCUMENT AI INTEGRATION

### Purpose:
- Extract text t·ª´ PDF documents (especially scanned images)
- Google Cloud Document AI Processor

### Configuration:

Stored in MongoDB collection `ai_config`:

```json
{
  "id": "system_ai",
  "provider": "google",
  "model": "gemini-2.0-flash",
  "use_emergent_key": true,
  "document_ai": {
    "enabled": true,
    "project_id": "YOUR_PROJECT_ID",
    "processor_id": "YOUR_PROCESSOR_ID",
    "location": "us"
  }
}
```

### How it's called:

```python
# V1: Through dual_apps_script_manager
from dual_apps_script_manager import create_dual_apps_script_manager

dual_manager = create_dual_apps_script_manager(company_uuid)

analysis_only_result = await dual_manager.analyze_survey_report_only(
    file_content=file_content,
    filename=filename,
    content_type=content_type,
    document_ai_config=document_ai_config
)

# Result structure:
{
    'success': True,
    'ai_analysis': {
        'success': True,
        'data': {
            'summary': 'This document is a survey record...'  # Full text extracted
        }
    }
}
```

### Document AI Output:

Document AI tr·∫£ v·ªÅ **plain text summary** c·ªßa to√†n b·ªô PDF document. ƒê√¢y l√† text ƒë√£ ƒë∆∞·ª£c OCR n·∫øu PDF l√† scanned image.

**Example output:**

```
This document is a survey record, authorization number A/25/772, from PMDS for 
cargo gear, derricks, cranes, ramps, and personal/cargo lifts.

Ship Name: MV EXAMPLE SHIP
IMO Number: 9876543
Classification Society: Lloyd's Register
Survey Date: 15 January 2024
Surveyor: John Smith

Equipment Inspected:
- Cargo Gear: Satisfactory condition
- Derrick #1: No defects noted
- Crane #2: Annual service completed
...
```

---

## 8. TARGETED OCR PROCESS

### Purpose:
- Extract Report Form v√† Report No t·ª´ **header/footer** c·ªßa PDF
- Complement Document AI (v√¨ Document AI c√≥ th·ªÉ miss c√°c th√¥ng tin ·ªü header/footer)

### Technology:
- **Tesseract OCR**: Open-source OCR engine
- **pdf2image**: Convert PDF pages to images
- **OpenCV (cv2)**: Image preprocessing
- **PIL (Pillow)**: Image manipulation

### File:
- **V1**: `/app/backend-v1/targeted_ocr.py`
- **M·ªõi**: `/app/backend/app/utils/targeted_ocr.py`

### Process:

```python
from targeted_ocr import get_ocr_processor

ocr_processor = get_ocr_processor()

# Check if OCR is available (requires Tesseract + pdf2image)
if ocr_processor.is_available():
    # Extract from first page (page_num=0)
    ocr_result = ocr_processor.extract_from_pdf(file_content, page_num=0)
    
    if ocr_result.get('ocr_success'):
        header_text = ocr_result.get('header_text', '').strip()
        footer_text = ocr_result.get('footer_text', '').strip()
        
        # Create OCR section
        ocr_section = "\n\n" + "="*60 + "\n"
        ocr_section += "ADDITIONAL INFORMATION FROM HEADER/FOOTER (OCR Extraction)\n"
        ocr_section += "="*60 + "\n\n"
        
        if header_text:
            ocr_section += "=== HEADER TEXT (Top 15% of page) ===\n"
            ocr_section += header_text + "\n\n"
        
        if footer_text:
            ocr_section += "=== FOOTER TEXT (Bottom 15% of page) ===\n"
            ocr_section += footer_text + "\n\n"
        
        ocr_section += "="*60
```

### TargetedOCRProcessor Implementation:

```python
class TargetedOCRProcessor:
    def __init__(self, header_percent: float = 0.15, footer_percent: float = 0.15):
        """
        Extract Report Form and Report No from header/footer regions
        
        Args:
            header_percent: Top 15% of page
            footer_percent: Bottom 15% of page
        """
        self.header_percent = header_percent
        self.footer_percent = footer_percent
    
    def extract_from_pdf(self, pdf_content: bytes, page_num: int = 0) -> Dict:
        """
        Extract from first page of PDF
        
        Returns:
            {
                'report_form': str or None,
                'survey_report_no': str or None,
                'header_text': str,
                'footer_text': str,
                'ocr_success': bool,
                'ocr_error': str or None
            }
        """
        
        # Step 1: Convert PDF to image (high DPI for better OCR)
        from pdf2image import convert_from_bytes
        
        images = convert_from_bytes(
            pdf_content,
            first_page=page_num + 1,
            last_page=page_num + 1,
            dpi=300  # High DPI for better accuracy
        )
        
        page_image = images[0]
        
        # Step 2: Extract header and footer text
        header_text = self._extract_header(page_image)
        footer_text = self._extract_footer(page_image)
        
        # Step 3: Extract fields using pattern matching
        combined_text = header_text + "\n" + footer_text
        
        report_form = self._extract_report_form(combined_text)
        survey_report_no = self._extract_report_no(combined_text)
        
        return {
            'report_form': report_form,
            'survey_report_no': survey_report_no,
            'header_text': header_text,
            'footer_text': footer_text,
            'ocr_success': True,
            'ocr_error': None
        }
    
    def _extract_header(self, image: Image) -> str:
        """Extract text from header region (top 15%)"""
        width, height = image.size
        header_height = int(height * self.header_percent)
        
        # Crop header region
        header_region = image.crop((0, 0, width, header_height))
        
        # Preprocess for better OCR
        header_region = self._preprocess_image(header_region)
        
        # OCR
        import pytesseract
        text = pytesseract.image_to_string(
            header_region,
            lang='eng',
            config='--psm 6 --oem 3'  # Assume uniform block of text
        )
        
        return text.strip()
    
    def _extract_footer(self, image: Image) -> str:
        """Extract text from footer region (bottom 15%)"""
        width, height = image.size
        footer_height = int(height * self.footer_percent)
        footer_y = height - footer_height
        
        # Crop footer region
        footer_region = image.crop((0, footer_y, width, height))
        
        # Preprocess
        footer_region = self._preprocess_image(footer_region)
        
        # OCR
        import pytesseract
        text = pytesseract.image_to_string(
            footer_region,
            lang='eng',
            config='--psm 6 --oem 3'
        )
        
        return text.strip()
    
    def _preprocess_image(self, image: Image) -> Image:
        """
        Preprocess image for better OCR accuracy
        - Grayscale conversion
        - Denoising
        - Binarization (threshold)
        """
        import cv2
        import numpy as np
        
        # Convert PIL Image to numpy array
        img_array = np.array(image)
        
        # Convert to grayscale
        if len(img_array.shape) == 3:
            gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
        else:
            gray = img_array
        
        # Denoise
        denoised = cv2.fastNlMeansDenoising(gray, h=10)
        
        # Threshold (Otsu's method for automatic threshold)
        _, thresh = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # Convert back to PIL Image
        processed_image = Image.fromarray(thresh)
        
        return processed_image
    
    def _extract_report_form(self, text: str) -> Optional[str]:
        """
        Extract Report Form using regex patterns
        
        Common patterns:
        - Form No.: XXX
        - Form Type: XXX
        - Report Form: XXX
        """
        patterns = [
            r'Form\s*No[.:]\s*([A-Z0-9\-/\s]+)',
            r'Form\s*Type[.:]\s*([A-Z0-9\-/\s]+)',
            r'Form[:\s]+([A-Z0-9\-/\s]+)',
            r'Report\s*Form[.:]\s*([A-Z0-9\-/\s]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                form = match.group(1).strip()
                form = re.sub(r'\s+', ' ', form)  # Clean whitespace
                if len(form) <= 50:
                    return form
        
        return None
    
    def _extract_report_no(self, text: str) -> Optional[str]:
        """
        Extract Survey Report No. using regex patterns
        
        Common patterns:
        - Report No.: A/25/772
        - Authorization No.: A/25/772
        - Survey Report No.: XXX
        """
        patterns = [
            r'Survey\s*Report\s*No[.:]\s*([A-Z0-9\-/]+)',
            r'Report\s*No[.:]\s*([A-Z0-9\-/]+)',
            r'Authorization\s*No[.:]\s*([A-Z0-9\-/]+)',
            r'Authorization\s*Number[.:]\s*([A-Z0-9\-/]+)',
            r'No[.:]\s*([A-Z0-9\-/]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)
            if match:
                report_no = match.group(1).strip()
                if len(report_no) <= 30 and re.search(r'[A-Z0-9]', report_no):
                    return report_no
        
        return None
```

### OCR Enhancement trong Summary:

Sau khi OCR extract ƒë∆∞·ª£c header/footer text, n√≥ ƒë∆∞·ª£c merge v√†o Document AI summary:

```python
# If Document AI succeeded
if summary_text:
    if ocr_section:
        # Merge: Document AI summary + OCR section
        summary_text = summary_text + ocr_section
        logger.info(f"Enhanced summary with OCR: {len(summary_text)} chars")

# If Document AI failed
else:
    # Use OCR only
    summary_text = ocr_section if ocr_section else ''
```

**Enhanced summary structure:**

```
[Document AI Summary - Full text from PDF]

=============================================================
ADDITIONAL INFORMATION FROM HEADER/FOOTER (OCR Extraction)
=============================================================

=== HEADER TEXT (Top 15% of page) ===
Form No.: CG (02-19)
Report No.: A/25/772

=== FOOTER TEXT (Bottom 15% of page) ===
Classification Society: Lloyd's Register
Surveyor: John Smith

=============================================================
Note: The above header/footer text was extracted using OCR
and may contain critical information like Report Form and Report No.
=============================================================
```

---

## 9. PDF SPLITTING LOGIC (>15 PAGES)

### Purpose:
- Google Document AI c√≥ gi·ªõi h·∫°n **15 pages per request**
- PDFs c√≥ >15 pages ph·∫£i ƒë∆∞·ª£c split th√†nh chunks nh·ªè h∆°n

### File:
- **V1**: `/app/backend-v1/pdf_splitter.py`
- **M·ªõi**: `/app/backend/app/utils/pdf_splitter.py`

### PDFSplitter Class:

```python
class PDFSplitter:
    def __init__(self, max_pages_per_chunk: int = 12):
        """
        Args:
            max_pages_per_chunk: Max pages per chunk (default 12, safely under 15 limit)
        """
        self.max_pages_per_chunk = max_pages_per_chunk
    
    def get_page_count(self, pdf_content: bytes) -> int:
        """Get total page count of PDF"""
        import PyPDF2
        import io
        
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))
            return len(pdf_reader.pages)
        except Exception as e:
            raise ValueError(f"Invalid PDF file: {str(e)}")
    
    def needs_splitting(self, pdf_content: bytes) -> bool:
        """Check if PDF needs to be split (> 15 pages)"""
        page_count = self.get_page_count(pdf_content)
        return page_count > 15
    
    def split_pdf(self, pdf_content: bytes, filename: str) -> List[Dict]:
        """
        Split PDF into chunks
        
        Returns:
            [
                {
                    'content': bytes,
                    'chunk_num': 1,
                    'page_range': '1-12',
                    'start_page': 1,
                    'end_page': 12,
                    'page_count': 12,
                    'filename': 'survey_report_chunk1.pdf',
                    'size_bytes': 12345
                },
                ...
            ]
        """
        import PyPDF2
        import io
        
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_content))
        total_pages = len(pdf_reader.pages)
        
        if total_pages <= 15:
            # No splitting needed
            return [{
                'content': pdf_content,
                'chunk_num': 1,
                'page_range': f'1-{total_pages}',
                'start_page': 1,
                'end_page': total_pages,
                'page_count': total_pages,
                'filename': filename,
                'size_bytes': len(pdf_content)
            }]
        
        logger.info(f"Splitting PDF: {filename} ({total_pages} pages) into chunks of {self.max_pages_per_chunk}")
        
        chunks = []
        base_filename = filename.rsplit('.', 1)[0]
        
        for start_page in range(0, total_pages, self.max_pages_per_chunk):
            end_page = min(start_page + self.max_pages_per_chunk, total_pages)
            chunk_num = len(chunks) + 1
            
            # Create new PDF for this chunk
            pdf_writer = PyPDF2.PdfWriter()
            
            for page_num in range(start_page, end_page):
                pdf_writer.add_page(pdf_reader.pages[page_num])
            
            # Write to bytes
            chunk_bytes_io = io.BytesIO()
            pdf_writer.write(chunk_bytes_io)
            chunk_content = chunk_bytes_io.getvalue()
            
            chunk_info = {
                'content': chunk_content,
                'chunk_num': chunk_num,
                'page_range': f'{start_page + 1}-{end_page}',
                'start_page': start_page + 1,
                'end_page': end_page,
                'page_count': end_page - start_page,
                'filename': f'{base_filename}_chunk{chunk_num}.pdf',
                'size_bytes': len(chunk_content)
            }
            
            chunks.append(chunk_info)
            logger.info(f"Chunk {chunk_num}: pages {chunk_info['page_range']}, size: {chunk_info['size_bytes']} bytes")
        
        logger.info(f"Split complete: {len(chunks)} chunks created")
        return chunks
```

### Process Large PDF (>15 pages):

```python
# Example: 30 page PDF

# Step 1: Split
chunks = splitter.split_pdf(file_content, filename)
# Result: 3 chunks
# - Chunk 1: pages 1-12 (12 pages)
# - Chunk 2: pages 13-24 (12 pages)
# - Chunk 3: pages 25-30 (6 pages)

# Step 2: Process each chunk with Document AI
chunk_results = []
for i, chunk in enumerate(chunks):
    chunk_analysis = await dual_manager.analyze_survey_report_only(
        file_content=chunk['content'],
        filename=chunk['filename'],
        content_type='application/pdf',
        document_ai_config=document_ai_config
    )
    
    # Collect summaries only (no field extraction yet)
    if chunk_analysis.get('success'):
        ai_analysis = chunk_analysis.get('ai_analysis', {})
        if ai_analysis.get('success'):
            summary_text = ai_analysis.get('data', {}).get('summary', '')
            
            chunk_results.append({
                'success': True,
                'chunk_num': chunk['chunk_num'],
                'page_range': chunk['page_range'],
                'summary_text': summary_text
            })

# Step 3: Merge summaries
from pdf_splitter import create_enhanced_merged_summary

merged_summary_text = create_enhanced_merged_summary(
    chunk_results=chunk_results,
    merged_data=temp_merged_data,  # Will be updated later
    original_filename=filename,
    total_pages=total_pages
)

# Step 4: Perform Targeted OCR on FIRST CHUNK only
first_chunk_content = chunks[0]['content']
ocr_result = ocr_processor.extract_from_pdf(first_chunk_content, page_num=0)

# Merge OCR into merged summary
if ocr_result.get('ocr_success'):
    merged_summary_text = merged_summary_text + ocr_section

# Step 5: Extract fields from merged summary (1 time only)
extracted_fields = await extract_survey_report_fields_from_summary(
    merged_summary_text,
    ai_provider,
    ai_model,
    use_emergent_key,
    filename
)

# Step 6: Recreate final summary with actual extracted data
final_merged_summary = create_enhanced_merged_summary(
    chunk_results=chunk_results,
    merged_data=extracted_fields,  # Now we have actual data
    original_filename=filename,
    total_pages=total_pages
)

# Result
analysis_result.update(extracted_fields)
analysis_result['_summary_text'] = final_merged_summary
analysis_result['_split_info'] = {
    'was_split': True,
    'total_pages': total_pages,
    'chunks_count': len(chunks),
    'successful_chunks': len([cr for cr in chunk_results if cr.get('success')])
}
```

### Merge Strategy:

```python
def create_enhanced_merged_summary(
    chunk_results: List[Dict],
    merged_data: Dict,
    original_filename: str,
    total_pages: int
) -> str:
    """
    Create enhanced merged summary from chunk results
    
    Structure:
    1. Header with metadata
    2. Key extracted fields
    3. Individual chunk summaries
    4. Footer
    """
    
    summary_parts = []
    
    # Header
    summary_parts.append("="*80)
    summary_parts.append("MERGED SURVEY REPORT ANALYSIS")
    summary_parts.append("="*80)
    summary_parts.append(f"Original File: {original_filename}")
    summary_parts.append(f"Total Pages: {total_pages}")
    summary_parts.append(f"Chunks Processed: {len(chunk_results)}")
    summary_parts.append("")
    
    # Key Fields
    summary_parts.append("--- KEY EXTRACTED FIELDS ---")
    summary_parts.append(f"Survey Report Name: {merged_data.get('survey_report_name', 'N/A')}")
    summary_parts.append(f"Report Form: {merged_data.get('report_form', 'N/A')}")
    summary_parts.append(f"Report No: {merged_data.get('survey_report_no', 'N/A')}")
    summary_parts.append(f"Issued By: {merged_data.get('issued_by', 'N/A')}")
    summary_parts.append(f"Issued Date: {merged_data.get('issued_date', 'N/A')}")
    summary_parts.append(f"Surveyor: {merged_data.get('surveyor_name', 'N/A')}")
    summary_parts.append("")
    
    # Individual Chunk Summaries
    summary_parts.append("="*80)
    summary_parts.append("DETAILED CONTENT FROM EACH CHUNK")
    summary_parts.append("="*80)
    
    for chunk_result in chunk_results:
        if chunk_result.get('success'):
            chunk_num = chunk_result.get('chunk_num', 0)
            page_range = chunk_result.get('page_range', '')
            summary_text = chunk_result.get('summary_text', '')
            
            summary_parts.append("")
            summary_parts.append(f"--- CHUNK {chunk_num} (Pages {page_range}) ---")
            summary_parts.append(summary_text)
            summary_parts.append("")
    
    # Footer
    summary_parts.append("="*80)
    summary_parts.append("END OF MERGED SUMMARY")
    summary_parts.append("="*80)
    
    return "\n".join(summary_parts)
```

**Example merged summary:**

```
================================================================================
MERGED SURVEY REPORT ANALYSIS
================================================================================
Original File: BWM-CHECK_LIST_11-23.pdf
Total Pages: 30
Chunks Processed: 3

--- KEY EXTRACTED FIELDS ---
Survey Report Name: cargo gear
Report Form: CG (02-19)
Report No: A/25/772
Issued By: Lloyd's Register
Issued Date: 2024-01-15
Surveyor: John Smith

================================================================================
DETAILED CONTENT FROM EACH CHUNK
================================================================================

--- CHUNK 1 (Pages 1-12) ---
This document is a survey record, authorization number A/25/772, from PMDS for 
cargo gear, derricks, cranes, ramps, and personal/cargo lifts.

Ship Name: MV EXAMPLE SHIP
IMO Number: 9876543
...

--- CHUNK 2 (Pages 13-24) ---
Continued inspection of cargo gear and lifting equipment.

Derrick #1 - Annual service completed. All components in satisfactory condition.
Derrick #2 - Wire rope replaced. Load test performed successfully.
...

--- CHUNK 3 (Pages 25-30) ---
Final inspection notes and recommendations.

All cargo gear found in satisfactory condition. No deficiencies noted.
...

================================================================================
END OF MERGED SUMMARY
================================================================================
```

---

## 10. FILE UPLOAD TO GOOGLE DRIVE

### Purpose:
- Upload 2 files to Google Drive:
  1. **Original PDF file** ‚Üí `ShipName/Class & Flag Cert/Class Survey Report/[filename]`
  2. **Summary text file** ‚Üí `SUMMARY/Class & Flag Document/[filename]_Summary.txt`

### Technology:
- **Google Apps Script** (deployed as web app)
- Endpoint URL stored in company settings

### V1 Implementation:

```python
# File: dual_apps_script_manager.py

class DualAppsScriptManager:
    def __init__(self, company_uuid: str):
        self.company_uuid = company_uuid
        # Get Apps Script URL from company settings
        self.apps_script_url = self._get_apps_script_url()
    
    async def upload_survey_report_file(
        self,
        file_content: bytes,
        filename: str,
        content_type: str,
        ship_name: str,
        survey_report_name: str
    ) -> Dict[str, Any]:
        """
        Upload original survey report file to Google Drive
        
        Target path: ShipName/Class & Flag Cert/Class Survey Report/[filename]
        """
        
        # Encode file to base64
        file_base64 = base64.b64encode(file_content).decode('utf-8')
        
        # Prepare request
        payload = {
            'action': 'uploadSurveyReportFile',
            'fileName': filename,
            'fileContent': file_base64,
            'contentType': content_type,
            'shipName': ship_name,
            'surveyReportName': survey_report_name
        }
        
        # Call Apps Script
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.apps_script_url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
        
        if result.get('success'):
            return {
                'success': True,
                'survey_report_file_id': result.get('fileId'),
                'file_url': result.get('fileUrl'),
                'message': 'Survey report file uploaded successfully'
            }
        else:
            return {
                'success': False,
                'message': result.get('error', 'Unknown error')
            }
    
    async def upload_survey_report_summary(
        self,
        summary_text: str,
        filename: str,
        ship_name: str
    ) -> Dict[str, Any]:
        """
        Upload summary text file to Google Drive
        
        Target path: SUMMARY/Class & Flag Document/[filename]_Summary.txt
        """
        
        # Prepare request
        payload = {
            'action': 'uploadSurveyReportSummary',
            'summaryText': summary_text,
            'fileName': filename,
            'shipName': ship_name
        }
        
        # Call Apps Script
        async with aiohttp.ClientSession() as session:
            async with session.post(
                self.apps_script_url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                result = await response.json()
        
        if result.get('success'):
            return {
                'success': True,
                'summary_file_id': result.get('fileId'),
                'file_url': result.get('fileUrl'),
                'message': 'Summary file uploaded successfully'
            }
        else:
            return {
                'success': False,
                'message': result.get('error', 'Unknown error')
            }
```

### Google Apps Script (Simplified):

```javascript
// Apps Script Web App endpoint

function doPost(e) {
  try {
    const data = JSON.parse(e.postData.contents);
    const action = data.action;
    
    if (action === 'uploadSurveyReportFile') {
      return uploadSurveyReportFile(data);
    } else if (action === 'uploadSurveyReportSummary') {
      return uploadSurveyReportSummary(data);
    }
    
    return ContentService.createTextOutput(JSON.stringify({
      success: false,
      error: 'Unknown action'
    })).setMimeType(ContentService.MimeType.JSON);
    
  } catch (error) {
    return ContentService.createTextOutput(JSON.stringify({
      success: false,
      error: error.toString()
    })).setMimeType(ContentService.MimeType.JSON);
  }
}

function uploadSurveyReportFile(data) {
  const fileName = data.fileName;
  const fileContent = data.fileContent;  // base64
  const contentType = data.contentType;
  const shipName = data.shipName;
  
  // Decode base64
  const fileBlob = Utilities.newBlob(
    Utilities.base64Decode(fileContent),
    contentType,
    fileName
  );
  
  // Get or create folder structure
  // Root ‚Üí ShipName ‚Üí Class & Flag Cert ‚Üí Class Survey Report
  const rootFolder = DriveApp.getFolderById('ROOT_FOLDER_ID');
  const shipFolder = getOrCreateFolder(rootFolder, shipName);
  const certFolder = getOrCreateFolder(shipFolder, 'Class & Flag Cert');
  const surveyFolder = getOrCreateFolder(certFolder, 'Class Survey Report');
  
  // Upload file
  const uploadedFile = surveyFolder.createFile(fileBlob);
  
  // Set permissions (optional)
  // uploadedFile.setSharing(DriveApp.Access.ANYONE_WITH_LINK, DriveApp.Permission.VIEW);
  
  return ContentService.createTextOutput(JSON.stringify({
    success: true,
    fileId: uploadedFile.getId(),
    fileUrl: uploadedFile.getUrl()
  })).setMimeType(ContentService.MimeType.JSON);
}

function uploadSurveyReportSummary(data) {
  const summaryText = data.summaryText;
  const fileName = data.fileName;
  const shipName = data.shipName;
  
  // Get or create folder structure
  // Root ‚Üí SUMMARY ‚Üí Class & Flag Document
  const rootFolder = DriveApp.getFolderById('ROOT_FOLDER_ID');
  const summaryFolder = getOrCreateFolder(rootFolder, 'SUMMARY');
  const classFlagFolder = getOrCreateFolder(summaryFolder, 'Class & Flag Document');
  
  // Create text file
  const textBlob = Utilities.newBlob(summaryText, 'text/plain', fileName);
  const uploadedFile = classFlagFolder.createFile(textBlob);
  
  return ContentService.createTextOutput(JSON.stringify({
    success: true,
    fileId: uploadedFile.getId(),
    fileUrl: uploadedFile.getUrl()
  })).setMimeType(ContentService.MimeType.JSON);
}

function getOrCreateFolder(parentFolder, folderName) {
  const folders = parentFolder.getFoldersByName(folderName);
  if (folders.hasNext()) {
    return folders.next();
  } else {
    return parentFolder.createFolder(folderName);
  }
}
```

### Backend M·ªõi Implementation:

```python
# File: app/services/gdrive_service.py

class GoogleDriveService:
    """Service for Google Drive operations"""
    
    @staticmethod
    async def upload_survey_report_file(
        company_uuid: str,
        file_content: bytes,
        filename: str,
        content_type: str,
        ship_name: str,
        survey_report_name: str
    ) -> Dict[str, Any]:
        """
        Upload original survey report file to Google Drive
        
        Uses company's Google Apps Script URL
        """
        
        # Get company settings
        company = await mongo_db.find_one("companies", {"id": company_uuid})
        if not company:
            raise HTTPException(404, "Company not found")
        
        apps_script_url = company.get("google_drive_apps_script_url")
        if not apps_script_url:
            raise HTTPException(400, "Google Drive not configured for this company")
        
        # Encode file to base64
        file_base64 = base64.b64encode(file_content).decode('utf-8')
        
        # Prepare request
        payload = {
            'action': 'uploadSurveyReportFile',
            'fileName': filename,
            'fileContent': file_base64,
            'contentType': content_type,
            'shipName': ship_name,
            'surveyReportName': survey_report_name
        }
        
        # Call Apps Script
        import aiohttp
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                apps_script_url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                result = await response.json()
        
        if result.get('success'):
            return {
                'success': True,
                'survey_report_file_id': result.get('fileId'),
                'file_url': result.get('fileUrl')
            }
        else:
            return {
                'success': False,
                'error': result.get('error', 'Upload failed')
            }
    
    @staticmethod
    async def upload_survey_report_summary(
        company_uuid: str,
        summary_text: str,
        filename: str,
        ship_name: str
    ) -> Dict[str, Any]:
        """
        Upload summary text file to Google Drive
        """
        
        # Get company settings
        company = await mongo_db.find_one("companies", {"id": company_uuid})
        apps_script_url = company.get("google_drive_apps_script_url")
        
        # Prepare request
        payload = {
            'action': 'uploadSurveyReportSummary',
            'summaryText': summary_text,
            'fileName': filename,
            'shipName': ship_name
        }
        
        # Call Apps Script
        import aiohttp
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                apps_script_url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                result = await response.json()
        
        if result.get('success'):
            return {
                'success': True,
                'summary_file_id': result.get('fileId'),
                'file_url': result.get('fileUrl')
            }
        else:
            return {
                'success': False,
                'error': result.get('error', 'Upload failed')
            }
```

### File Deletion on Google Drive:

Khi survey report b·ªã x√≥a, files tr√™n Google Drive c≈©ng ph·∫£i b·ªã x√≥a.

```python
# V1: backend-v1/server.py

@api_router.delete("/survey-reports/{report_id}")
async def delete_survey_report(
    report_id: str,
    background_tasks: BackgroundTasks,
    current_user: UserResponse = Depends(check_permission([...]))
):
    """
    Delete survey report
    1. Delete from database
    2. Delete files from Google Drive (in background)
    """
    
    # Get report
    report = await mongo_db.find_one("survey_reports", {"id": report_id})
    if not report:
        raise HTTPException(404, "Survey report not found")
    
    # Extract file IDs
    survey_report_file_id = report.get("survey_report_file_id")
    survey_report_summary_file_id = report.get("survey_report_summary_file_id")
    
    # Delete from database
    await mongo_db.delete("survey_reports", {"id": report_id})
    
    # Delete files from Google Drive in background
    if survey_report_file_id or survey_report_summary_file_id:
        from dual_apps_script_manager import create_dual_apps_script_manager
        
        dual_manager = create_dual_apps_script_manager(company_uuid)
        
        # Add background task
        background_tasks.add_task(
            dual_manager.delete_files,
            file_ids=[survey_report_file_id, survey_report_summary_file_id]
        )
    
    return {
        "success": True,
        "message": "Survey report deleted successfully"
    }
```

### Bulk Delete:

```python
@api_router.post("/survey-reports/bulk-delete")
async def bulk_delete_survey_reports(
    request: BulkDeleteSurveyReportRequest,
    background_tasks: BackgroundTasks,
    current_user: UserResponse = Depends(check_permission([...]))
):
    """
    Bulk delete survey reports
    1. Delete from database
    2. Delete files from Google Drive (in background)
    """
    
    report_ids = request.report_ids
    
    if not report_ids or len(report_ids) == 0:
        raise HTTPException(400, "No report IDs provided")
    
    # Get all reports
    reports = await mongo_db.find_all("survey_reports", {"id": {"$in": report_ids}})
    
    # Collect file IDs
    file_ids_to_delete = []
    for report in reports:
        if report.get("survey_report_file_id"):
            file_ids_to_delete.append(report["survey_report_file_id"])
        if report.get("survey_report_summary_file_id"):
            file_ids_to_delete.append(report["survey_report_summary_file_id"])
    
    # Delete from database
    result = await mongo_db.delete_many("survey_reports", {"id": {"$in": report_ids}})
    deleted_count = result.deleted_count
    
    # Delete files from Google Drive in background
    if file_ids_to_delete:
        from dual_apps_script_manager import create_dual_apps_script_manager
        
        dual_manager = create_dual_apps_script_manager(company_uuid)
        
        background_tasks.add_task(
            dual_manager.delete_files,
            file_ids=file_ids_to_delete
        )
    
    return {
        "success": True,
        "deleted_count": deleted_count,
        "message": f"Deleted {deleted_count} survey reports"
    }
```

---

## 11. SO S√ÅNH BACKEND V1 VS BACKEND M·ªöI

### A. C·∫•u tr√∫c code:

| Aspect | Backend V1 | Backend M·ªõi |
|--------|-----------|-------------|
| **File structure** | Monolithic (1 file `server.py` ~10,000 lines) | Modular (separate files for routes, services, models, utils) |
| **Endpoints** | All in `server.py` | Separated in `api/v1/survey_reports.py` |
| **Business logic** | Mixed with endpoints in `server.py` | Separated in `services/` folder |
| **AI logic** | Mixed with endpoint logic | Separated in `utils/survey_report_ai.py` |
| **Models** | Inline Pydantic models | Separated in `models/survey_report.py` |
| **Google Drive** | `dual_apps_script_manager.py` | `services/gdrive_service.py` |

### B. Features parity:

| Feature | Backend V1 | Backend M·ªõi | Status |
|---------|-----------|-------------|--------|
| **PDF validation** | ‚úÖ Magic bytes check | ‚úÖ Same | ‚úÖ Parity |
| **PDF splitting (>15 pages)** | ‚úÖ `pdf_splitter.py` | ‚úÖ `utils/pdf_splitter.py` | ‚úÖ Parity |
| **Document AI integration** | ‚úÖ Through `dual_manager` | ‚úÖ Through `document_ai_service` | ‚úÖ Parity |
| **Targeted OCR** | ‚úÖ `targeted_ocr.py` | ‚úÖ `utils/targeted_ocr.py` | ‚úÖ Parity |
| **System AI extraction** | ‚úÖ LLM prompt | ‚úÖ Same prompt in `survey_report_ai.py` | ‚úÖ Parity |
| **Ship validation** | ‚úÖ Fuzzy matching | ‚úÖ Same logic | ‚úÖ Parity |
| **File upload (original + summary)** | ‚úÖ Via Apps Script | ‚úÖ Via `gdrive_service` | ‚úÖ Parity |
| **File deletion on record delete** | ‚úÖ Background task | ‚úÖ Background task | ‚úÖ Parity |
| **Bulk delete** | ‚úÖ POST endpoint | ‚úÖ POST endpoint | ‚úÖ Parity |
| **Response models** | ‚úÖ Inline Pydantic | ‚úÖ Separated in `models/` | ‚úÖ Parity |
| **Field name mapping** | ‚úÖ Correct (`survey_report_file_id`) | ‚úÖ Fixed bug (was `file_id`) | ‚úÖ Parity |

### C. Key differences:

#### 1. Code organization:

**V1 (Monolithic):**
```
backend-v1/
‚îú‚îÄ‚îÄ server.py                    # 10,000+ lines
‚îú‚îÄ‚îÄ targeted_ocr.py
‚îú‚îÄ‚îÄ pdf_splitter.py
‚îú‚îÄ‚îÄ dual_apps_script_manager.py
‚îî‚îÄ‚îÄ ...
```

**M·ªõi (Modular):**
```
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/v1/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ survey_reports.py    # 189 lines (routes only)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ survey_report_service.py         # CRUD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ survey_report_analyze_service.py # AI analysis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gdrive_service.py                # Google Drive
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ survey_report_ai.py   # LLM prompt & extraction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_splitter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ targeted_ocr.py
‚îÇ   ‚îî‚îÄ‚îÄ models/
‚îÇ       ‚îî‚îÄ‚îÄ survey_report.py      # Pydantic models
```

#### 2. Endpoint delegation:

**V1:**
```python
# All logic in endpoint
@api_router.post("/survey-reports/analyze-file")
async def analyze_survey_report_file(...):
    # 700 lines of logic here
    ...
```

**M·ªõi:**
```python
# Endpoint delegates to service
@router.post("/analyze-file")
async def analyze_survey_report_file(...):
    from app.services.survey_report_analyze_service import SurveyReportAnalyzeService
    
    result = await SurveyReportAnalyzeService.analyze_file(
        file=survey_report_file,
        ship_id=ship_id,
        bypass_validation=bypass_bool,
        current_user=current_user
    )
    
    return result
```

#### 3. AI prompt location:

**V1:**
```python
# In server.py (line 7562-7709)
def create_survey_report_extraction_prompt(summary_text: str, filename: str = "") -> str:
    prompt = f"""You are an AI specialized in maritime survey report..."""
    return prompt
```

**M·ªõi:**
```python
# In app/utils/survey_report_ai.py
def create_survey_report_extraction_prompt(summary_text: str, filename: str = "") -> str:
    prompt = f"""You are an AI specialized in maritime survey report..."""
    return prompt
```

#### 4. Google Drive manager:

**V1:**
```python
# dual_apps_script_manager.py
class DualAppsScriptManager:
    async def upload_survey_report_file(...):
        ...
    
    async def upload_survey_report_summary(...):
        ...
    
    async def delete_files(...):
        ...
```

**M·ªõi:**
```python
# app/services/gdrive_service.py
class GoogleDriveService:
    @staticmethod
    async def upload_survey_report_file(...):
        ...
    
    @staticmethod
    async def upload_survey_report_summary(...):
        ...
    
    @staticmethod
    async def delete_files(...):
        ...
```

#### 5. Bug fixes trong migration:

**Bug 1: Field name mismatch**

V1: Correct field names
```python
# Response model has correct field names
class SurveyReportResponse(BaseModel):
    survey_report_file_id: Optional[str]
    survey_report_summary_file_id: Optional[str]
```

M·ªõi (Before fix): Wrong field names
```python
# Response model had wrong field names
class SurveyReportResponse(BaseModel):
    file_id: Optional[str]           # ‚ùå WRONG
    summary_file_id: Optional[str]   # ‚ùå WRONG
```

M·ªõi (After fix): Corrected
```python
# Fixed to match database
class SurveyReportResponse(BaseModel):
    survey_report_file_id: Optional[str]        # ‚úÖ CORRECT
    survey_report_summary_file_id: Optional[str] # ‚úÖ CORRECT
```

**Bug 2: Bulk delete HTTP method**

V1: POST (correct, allows body)
```python
@api_router.post("/survey-reports/bulk-delete")
```

M·ªõi (Before fix): DELETE (blocked by proxy)
```python
@router.delete("/bulk-delete")  # ‚ùå Blocked by firewall
```

M·ªõi (After fix): POST
```python
@router.post("/bulk-delete")    # ‚úÖ Works
```

**Bug 3: Background file deletion**

V1: Correct implementation
```python
@api_router.delete("/survey-reports/{report_id}")
async def delete_survey_report(
    report_id: str,
    background_tasks: BackgroundTasks,  # ‚úÖ Receives BackgroundTasks
    ...
):
    # Delete from DB
    await mongo_db.delete("survey_reports", {"id": report_id})
    
    # Delete files in background
    background_tasks.add_task(
        dual_manager.delete_files,
        file_ids=[...]
    )
```

M·ªõi (Before fix): Missing BackgroundTasks
```python
@router.delete("/{doc_id}")
async def delete_survey_report(
    doc_id: str,
    # ‚ùå Missing BackgroundTasks parameter
    ...
):
    # Files not deleted from Google Drive
```

M·ªõi (After fix): Correct
```python
@router.delete("/{doc_id}")
async def delete_survey_report(
    doc_id: str,
    background_tasks: BackgroundTasks,  # ‚úÖ Added
    ...
):
    # Delete files in background
    background_tasks.add_task(...)
```

### D. Performance comparison:

| Metric | Backend V1 | Backend M·ªõi | Notes |
|--------|-----------|-------------|-------|
| **Code readability** | ‚≠ê‚≠ê (monolithic) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (modular) | M·ªõi d·ªÖ ƒë·ªçc v√† maintain h∆°n nhi·ªÅu |
| **Code maintainability** | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Separation of concerns |
| **Testing** | ‚≠ê‚≠ê (hard to test) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (easy to unit test) | Services can be tested independently |
| **Runtime performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Same (logic is identical) |
| **API response time** | Same | Same | No difference |

---

## 12. T·ªîNG K·∫æT & KHUY·∫æN NGH·ªä

### A. T√≥m t·∫Øt flow "Add Survey Report":

```
1. Frontend: User ch·ªçn t√†u + upload PDF file

2. Frontend ‚Üí Backend: POST /api/survey-reports/analyze-file
   - FormData: survey_report_file, ship_id, bypass_validation

3. Backend: Analyze file
   3.1. Validate PDF (magic bytes, size, format)
   3.2. Check page count:
        - If ‚â§ 15 pages: Process entire document
        - If > 15 pages: Split into chunks (12 pages each)
   3.3. Process with Google Document AI:
        - Extract text summary from PDF
   3.4. Perform Targeted OCR:
        - Extract header/footer text (Report Form, Report No)
        - Use Tesseract OCR + pdf2image
   3.5. Merge summaries (if split)
   3.6. Extract fields with System AI (LLM):
        - Use enhanced prompt
        - Parse JSON response
        - Post-process fields (date validation, report_form from filename, etc.)
   3.7. Validate ship name/IMO:
        - Fuzzy matching with 80% threshold
        - If mismatch ‚Üí return validation_error
   3.8. Return analysis + file content (base64)

4. Frontend: Receive analysis
   4.1. Check for validation_error:
        - If exists ‚Üí Show modal for user confirmation
        - User can bypass or cancel
   4.2. Auto-fill form fields
   4.3. Store analyzed data (including hidden fields: _file_content, _summary_text)

5. Frontend: User clicks "Save"
   5.1. POST /api/survey-reports (create record without file IDs)
   5.2. Close modal
   5.3. Refresh list (first time - no file icons yet)

6. Frontend: Background upload
   6.1. POST /api/survey-reports/{report_id}/upload-files
        - Body: file_content (base64), filename, content_type, summary_text
   6.2. Backend uploads:
        - Original PDF ‚Üí Google Drive: ShipName/Class & Flag Cert/Class Survey Report/
        - Summary TXT ‚Üí Google Drive: SUMMARY/Class & Flag Document/
   6.3. Backend updates record with file IDs:
        - survey_report_file_id
        - survey_report_summary_file_id
   6.4. Frontend refreshes list (second time - file icons appear)
```

### B. C√°c components ch√≠nh:

1. **Frontend**:
   - `AddSurveyReportModal.jsx`: Main component
   - `surveyReportService.js`: API calls

2. **Backend Endpoints**:
   - `POST /api/survey-reports/analyze-file`: AI analysis
   - `POST /api/survey-reports`: Create record
   - `POST /api/survey-reports/{report_id}/upload-files`: Upload files
   - `DELETE /api/survey-reports/{report_id}`: Delete (v·ªõi file deletion)
   - `POST /api/survey-reports/bulk-delete`: Bulk delete

3. **Backend Services**:
   - `SurveyReportAnalyzeService`: AI analysis logic
   - `SurveyReportService`: CRUD operations
   - `GoogleDriveService`: File upload/delete

4. **Utils**:
   - `survey_report_ai.py`: LLM prompt & extraction
   - `pdf_splitter.py`: PDF splitting
   - `targeted_ocr.py`: OCR for header/footer

5. **External Integrations**:
   - Google Document AI: Text extraction
   - Tesseract OCR: Header/footer OCR
   - Google Apps Script: File upload to Drive
   - LLM (Gemini/GPT): Field extraction

### C. Migration status:

‚úÖ **ƒê√£ ho√†n th√†nh:**
- T·∫•t c·∫£ endpoints ƒë√£ ƒë∆∞·ª£c migrate
- T·∫•t c·∫£ logic ƒë√£ ƒë∆∞·ª£c migrate
- Bug fixes ƒë√£ ƒë∆∞·ª£c apply
- Feature parity ƒë√£ ƒë·∫°t ƒë∆∞·ª£c 100%

üîç **ƒê√£ test:**
- AI analysis (single PDF)
- AI analysis (split PDF > 15 pages)
- Targeted OCR
- Field extraction
- Ship validation
- File upload (original + summary)
- File deletion
- Bulk delete

‚úÖ **Bugs ƒë√£ fix:**
1. ‚úÖ Field name mismatch (`file_id` ‚Üí `survey_report_file_id`)
2. ‚úÖ Bulk delete HTTP method (DELETE ‚Üí POST)
3. ‚úÖ Background file deletion (th√™m BackgroundTasks)
4. ‚úÖ AI prompt improvement (d√πng V1 prompt chi ti·∫øt h∆°n)

### D. Khuy·∫øn ngh·ªã:

#### 1. Testing c·∫ßn th√™m:

a. **Multi-page PDF testing:**
   - Test v·ªõi PDFs c√≥ 15, 16, 20, 30, 50 pages
   - Verify merged summary c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin
   - Check extracted fields accuracy

b. **Edge cases:**
   - PDF v·ªõi scanned images (poor quality)
   - PDF v·ªõi nhi·ªÅu languages (English + Vietnamese)
   - PDF kh√¥ng c√≥ header/footer
   - PDF v·ªõi rotated pages
   - Corrupted PDFs

c. **Ship validation:**
   - Test v·ªõi ship names c√≥ special characters
   - Test v·ªõi IMO numbers c√≥ format kh√°c nhau
   - Test v·ªõi ship names r·∫•t gi·ªëng nhau (similarity 70-90%)

d. **Google Drive:**
   - Test v·ªõi large files (>20MB)
   - Test v·ªõi network interruptions
   - Test concurrent uploads
   - Verify folder structure creation

#### 2. Performance optimization:

a. **Caching:**
   - Cache Document AI results (n·∫øu c√πng file ƒë∆∞·ª£c upload l·∫°i)
   - Cache OCR results
   - Cache AI extraction results

b. **Parallel processing:**
   - Process multiple chunks in parallel (thay v√¨ sequential)
   - Use ThreadPoolExecutor ho·∫∑c asyncio.gather()

c. **File size limits:**
   - Current: 50MB per file
   - Consider: Chunked upload for larger files

#### 3. Monitoring & Logging:

a. **Add metrics:**
   - Document AI success rate
   - OCR success rate
   - Field extraction accuracy
   - File upload success rate
   - Average processing time

b. **Enhanced logging:**
   - Log all steps v·ªõi timestamps
   - Log AI prompts & responses
   - Log Google Drive operations
   - Log validation failures

#### 4. Error handling:

a. **Graceful degradation:**
   - If Document AI fails ‚Üí use OCR only
   - If OCR fails ‚Üí use Document AI only
   - If both fail ‚Üí allow manual entry

b. **Retry logic:**
   - Retry Document AI calls (v·ªõi exponential backoff)
   - Retry Google Drive uploads
   - Retry LLM calls

#### 5. User experience:

a. **Progress indicators:**
   - Show progress for large PDFs:
     - "Processing chunk 1/3..."
     - "Extracting text... 45%"
     - "Uploading files... 80%"

b. **Better error messages:**
   - Explain validation errors clearly
   - Suggest actions for users
   - Show AI confidence scores

c. **Preview:**
   - Show extracted text before submission
   - Allow users to review OCR results
   - Highlight confidence scores

### E. K·∫øt lu·∫≠n:

‚úÖ **Migration ƒë√£ HO√ÄN TH√ÄNH**
- Backend m·ªõi ƒë√£ ƒë·∫°t 100% feature parity v·ªõi backend-v1
- T·∫•t c·∫£ bugs ƒë√£ ƒë∆∞·ª£c fix
- Code structure ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán ƒë√°ng k·ªÉ (modular, testable, maintainable)

üöÄ **Next steps:**
1. ‚úÖ Full testing v·ªõi real-world PDFs
2. ‚úÖ Performance testing v·ªõi large files
3. ‚úÖ Load testing v·ªõi concurrent requests
4. ‚è≠Ô∏è Migration c√°c routes kh√°c t·ª´ backend-v1
5. ‚è≠Ô∏è Decommission backend-v1 sau khi ho√†n th√†nh 100% migration

---

**Ng√†y ho√†n th√†nh ph√¢n t√≠ch**: 2025
**Version**: 2.0
**Status**: ‚úÖ COMPLETE

